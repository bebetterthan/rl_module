# üéØ PHASE 1: Subfinder + HTTPX Sequential Strategy

## Objective

Master 2-TOOL SEQUENTIAL workflow:

1. **Subfinder** (subdomain discovery) ‚Üí Choose optimal mode
2. **HTTPX** (live probing) ‚Üí Choose optimal probe strategy

Learn which tool combinations work best for different target scenarios.

## Success Criteria

- ‚úÖ Agent beats random baseline by >100% (Target: >931 reward)
- ‚úÖ Agent beats hardcoded "always comprehensive" by >30% (Target: >857 reward)
- ‚úÖ Observable strategic decision-making (contextual tool selection)
- ‚úÖ Validates sandbox design + reward function

## Baselines (2-Tool Workflow)

- **Random**: 465.84 ¬± 214.11 (random subfinder + random httpx)
- **Hardcoded**: 659.21 ¬± 414.82 (always comprehensive for both)
- **Performance**: 25,043 steps/sec (25x target!)

## Timeline

**3 Days** | Local Training + TensorBoard Monitoring

## Structure

```
phase1_single_tool/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ TRAINING_GUIDE.md                  # üìä Local training & monitoring guide
‚îú‚îÄ‚îÄ train_local.py                     # üöÄ Main training script
‚îú‚îÄ‚îÄ start_tensorboard.py               # üìä TensorBoard launcher
‚îú‚îÄ‚îÄ outputs/                           # Training outputs
‚îÇ   ‚îî‚îÄ‚îÄ run_YYYYMMDD_HHMMSS/
‚îÇ       ‚îú‚îÄ‚îÄ tensorboard/               # TensorBoard logs
‚îÇ       ‚îú‚îÄ‚îÄ checkpoints/               # Model checkpoints
‚îÇ       ‚îú‚îÄ‚îÄ best_model/                # Best model
‚îÇ       ‚îú‚îÄ‚îÄ final_model.zip            # Final trained model
‚îÇ       ‚îî‚îÄ‚îÄ results.json               # Results summary
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ scenarios/
‚îÇ       ‚îú‚îÄ‚îÄ phase1_training.json       # 10 diverse training scenarios
‚îÇ       ‚îî‚îÄ‚îÄ phase1_eval.json           # 5 held-out test scenarios
‚îú‚îÄ‚îÄ baselines/
‚îÇ   ‚îú‚îÄ‚îÄ random_agent.py                # Random action baseline
‚îÇ   ‚îî‚îÄ‚îÄ hardcoded_agent.py             # Always comprehensive baseline
‚îú‚îÄ‚îÄ envs/
‚îÇ   ‚îî‚îÄ‚îÄ subfinder_env.py               # Gymnasium environment
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ test_subfinder_env.py          # Comprehensive test suite

```

## Current Status

### Day 1: Scenario Generation ‚úÖ COMPLETE

- ‚úÖ Generate 10 diverse training scenarios
- ‚úÖ Generate 5 held-out eval scenarios
- ‚úÖ Validate scenario diversity (PASS: 3/4/3 distribution)
- ‚úÖ Document diversity strategy

### Day 2: Environment Implementation ‚úÖ COMPLETE

- ‚úÖ Implement SubfinderEnv (15-dim state, 3 actions)
- ‚úÖ Implement reward function (4 components with anti-hacking)
- ‚úÖ Implement action masking
- ‚úÖ Test environment locally (**36,335 steps/sec** - 36x target!)
- ‚úÖ Implement baselines (random: 859¬±156, hardcoded: 681¬±164)
- ‚úÖ Comprehensive test suite (**17/17 tests passing**)

### Day 3: Local Training üîÑ READY TO START

- ‚è≥ Run training with PPO (100k timesteps, ~5-10 min)
- ‚è≥ Monitor with TensorBoard (localhost:6006)
- ‚è≥ Evaluate vs baselines
- ‚è≥ Check success criteria (>2x random, >1.3x hardcoded)
- ‚è≥ Document results

---

## üöÄ Quick Start

### 1. Run Training

```bash
cd phase1_single_tool
python train_local.py
```

### 2. Monitor (Separate Terminal)

```bash
cd phase1_single_tool
python start_tensorboard.py
# Or manually: tensorboard --logdir=outputs --port=6006
```

### 3. Open Browser

```
http://localhost:6006
```

### 4. Check Results

Training akan otomatis evaluate dan compare dengan baselines.
Results tersimpan di `outputs/run_YYYYMMDD_HHMMSS/results.json`

üìñ **Full Guide**: See `TRAINING_GUIDE.md` for detailed monitoring instructions.

---

## Design Philosophy (Gemini Insights)

### 80/20 Rule

- **80% Design** (scenarios + reward + state) ‚úÖ DONE
- **20% Execution** (just run and monitor) ‚è≥ NOW

### Critical Focus

1. **Scenario Diversity (40%)**: Pattern learning > Memorization ‚úÖ
2. **Reward Design (40%)**: Anti-reward-hacking measures ‚úÖ
3. **State Representation (15%)**: Rich, informative features ‚úÖ
4. **Training (5%)**: Run with good hyperparams ‚è≥

### Anti-Reward-Hacking Measures ‚úÖ

- Time penalty: Prevent spam
- Redundancy penalty: Prevent repeat scans
- Wrong tool penalty: Prevent always-comprehensive
- Strategic bonuses: Reward contextual decisions
- Completion multiplier: Encourage efficiency

---

## Next Steps

**RIGHT NOW**: Generate diverse scenarios with `generate_scenarios_phase1.py`

**Key**: Ensure REAL diversity (types, naming patterns, tech stacks, complexity)!
